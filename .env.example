# Required: absolute host path to your GGUF model file
LLM_MODEL_HOST_PATH=/absolute/path/to/your-model.gguf

# Vault (Markdown database)
# Local runs: set VAULT_PATH to where you want your notes stored (e.g., your Obsidian vault).
VAULT_PATH=/absolute/path/to/your-vault

# LLM provider
# - local: llama-cpp-python reading LLM_MODEL_PATH
# - openai: OpenAI-compatible API (OpenAI/OpenRouter/etc)
LLM_PROVIDER=local
LLM_API_KEY=
LLM_BASE_URL=
LLM_MODEL=your-model-id

# Whisper provider (speech-to-text)
# - local: faster-whisper (WHISPER_MODEL like "small", "medium", ...)
# - openai: OpenAI API (WHISPER_MODEL like "whisper-1")
WHISPER_PROVIDER=local
WHISPER_API_KEY=
WHISPER_BASE_URL=
WHISPER_MODEL=small

# Local GGUF mounted path inside container (local provider only)
LLM_MODEL_PATH=/models/model.gguf

# Context and performance tuning
LLM_CONTEXT_LENGTH=32768
LLM_N_GPU_LAYERS=30
LLM_THREADS=6
LLM_BATCH_SIZE=512
LLM_MAX_CONCURRENT_PREDICTIONS=4
LLM_MAX_TOKENS=2048

# Advanced toggles
LLM_FLASH_ATTENTION=true
LLM_USE_MMAP=true
LLM_OFFLOAD_KQV=true

# Optional deterministic seed (leave empty for random)
LLM_SEED=
