# Input
PROCESSOR_DATA_DIR=data

# Output vault
PROCESSOR_VAULT_DIR=vault

# LLM backend selection
LLM_PROVIDER=llama_cpp
LLM_CONCURRENCY=1

# Local GGUF mounted path
LLM_MODEL_PATH=/Users/joselu/.cache/lm-studio/models/unsloth/gemma-3n-E2B-it-GGUF/gemma-3n-E2B-it-Q4_K_S.gguf

# Context and performance tuning
LLM_CONTEXT_LENGTH=32768
LLM_N_GPU_LAYERS=30
LLM_THREADS=6
LLM_BATCH_SIZE=512
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.3
LLM_MAX_RETRIES=3

# Advanced toggles
LLM_FLASH_ATTENTION=true
LLM_USE_MMAP=true
LLM_OFFLOAD_KQV=true

# Optional deterministic seed (leave empty for random)
LLM_SEED=

# OpenAI
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# Gemini
GEMINI_API_KEY=
GEMINI_MODEL=gemini-2.0-flash

# Processing controls
PROCESSOR_MAX_EVENTS_PER_RUN=
PROCESSOR_OVERWRITE=0
